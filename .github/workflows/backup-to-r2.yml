name: Database Backup to R2 (AWS CLI + Gzip)

on:
  schedule:
    # åŒ—äº¬æ—¶é—´ 12:00 å’Œ 00:00 è¿è¡Œ
    - cron: '0 4,16 * * *'
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    env:
      # å¼ºåˆ¶æ•´ä¸ªçŽ¯å¢ƒä½¿ç”¨ä¸Šæµ·æ—¶é—´
      TZ: Asia/Shanghai

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install PostgreSQL Client
        run: |

          #ç”±äºŽ Postgres 17 è¾ƒæ–°ï¼ŒUbuntu é»˜è®¤æºå¯èƒ½æ²¡æœ‰ï¼Œéœ€æ‰‹åŠ¨æ·»åŠ å®˜æ–¹æº
          sudo apt-get install -y postgresql-common
          sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y
          # 2. å®‰è£… Client 17
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17

          # 3. å…³é”®ï¼šå°† 17 ç‰ˆæœ¬çš„è·¯å¾„åŠ åˆ°çŽ¯å¢ƒ PATH æœ€å‰é¢ï¼Œå¼ºåˆ¶æ›¿æ¢é»˜è®¤ç‰ˆæœ¬
          echo "/usr/lib/postgresql/17/bin" >> $GITHUB_PATH

      - name: ðŸ” Debug Database Content (Check if tables exist)
        env:
          DB_URL: ${{ secrets.DB_CONNECTION_STRING }}
        run: |
          echo "Checking database connectivity and content..."
          # åˆ—å‡ºæ‰€æœ‰è¡¨
          psql "$DB_URL" -c "\dt"
          
          # å°è¯•æŸ¥è¯¢æ˜¯å¦æœ‰æ•°æ® (å‡è®¾æœ‰ tokens è¡¨ï¼Œæ²¡æœ‰ä¼šæŠ¥é”™ä½†èƒ½å¸®æˆ‘ä»¬è¯Šæ–­)
          # || true ä¿è¯å³ä½¿æŸ¥è¯¢å¤±è´¥ä¹Ÿä¸ä¸­æ–­æµç¨‹
          echo "Checking row count in 'tokens' table:"
          psql "$DB_URL" -c "SELECT count(*) FROM tokens;" || echo "âš ï¸ Could not query tokens table"

      - name: Create Backup (Compressed)
        env:
          DB_URL: ${{ secrets.DB_CONNECTION_STRING }}
        run: |
          # é‡åˆ°é”™è¯¯ç«‹å³åœæ­¢ï¼Œé˜²æ­¢ä¸Šä¼ åæ–‡ä»¶
          set -euo pipefail

          # æ–‡ä»¶ååŽç¼€æ”¹ä¸º .sql.gz
          FILENAME="backup-$(date +'%Y%m%d-%H%M%S').sql.gz"
          echo "BACKUP_FILENAME=$FILENAME" >> $GITHUB_ENV

          # 1. å¯¼å‡º SQL (å¸¦æ¸…ç†é€»è¾‘)
          # 2. ç®¡é“ä¼ é€’ç»™ gzip åŽ‹ç¼© (-9 è¡¨ç¤ºæœ€é«˜åŽ‹ç¼©æ¯”)
          # 3. å†™å…¥æ–‡ä»¶
          pg_dump "$DB_URL" --format=plain --no-owner --no-acl --clean --if-exists | gzip -9 > "$FILENAME"

          echo "âœ… Backup created successfully: $FILENAME"

      - name: Upload to R2 using AWS CLI
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: auto
          AWS_EC2_METADATA_DISABLED: "true"
          R2_ENDPOINT: https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com
          BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          set -euo pipefail

          YEAR=$(date +'%Y')
          MONTH=$(date +'%m')
          S3_PATH="s3://${BUCKET_NAME}/${YEAR}/${MONTH}/${{ env.BACKUP_FILENAME }}"

          aws s3 cp "${{ env.BACKUP_FILENAME }}" "$S3_PATH" --endpoint-url "$R2_ENDPOINT"

          echo "ðŸš€ Backup uploaded to: $S3_PATH"

      - name: Cleanup Local File
        run: rm -f "${{ env.BACKUP_FILENAME }}"
