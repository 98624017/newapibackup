name: Database Backup to R2 (Multi-DB Support)

on:
  schedule:
    # åŒ—äº¬æ—¶é—´ 12:00 å’Œ 00:00 è¿è¡Œ
    - cron: '0 4,16 * * *'
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    env:
      # å¼ºåˆ¶æ•´ä¸ªç¯å¢ƒä½¿ç”¨ä¸Šæµ·æ—¶é—´
      TZ: Asia/Shanghai

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install PostgreSQL Client
        run: |
          #ç”±äº Postgres 17 è¾ƒæ–°ï¼ŒUbuntu é»˜è®¤æºå¯èƒ½æ²¡æœ‰ï¼Œéœ€æ‰‹åŠ¨æ·»åŠ å®˜æ–¹æº
          sudo apt-get install -y postgresql-common
          sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y
          # 2. å®‰è£… Client 17
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17
          
          # 3. å…³é”®ï¼šå°† 17 ç‰ˆæœ¬çš„è·¯å¾„åŠ åˆ°ç¯å¢ƒ PATH æœ€å‰é¢ï¼Œå¼ºåˆ¶æ›¿æ¢é»˜è®¤ç‰ˆæœ¬
          echo "/usr/lib/postgresql/17/bin" >> $GITHUB_PATH

      - name: Process Multiple Databases
        env:
          # JSON æ ¼å¼: [{"name": "db1", "url": "postgres://..."}, {"name": "db2", "url": "..."}]
          DB_BACKUP_CONFIG: ${{ secrets.DB_BACKUP_CONFIG }}
          # R2 é…ç½®
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: auto
          AWS_EC2_METADATA_DISABLED: "true"
          R2_ENDPOINT: https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com
          BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          set -e
          
          # æ£€æŸ¥é…ç½®æ˜¯å¦å­˜åœ¨
          if [ -z "$DB_BACKUP_CONFIG" ]; then
            echo "âŒ Error: DB_BACKUP_CONFIG secret is empty."
            exit 1
          fi

          # å‡†å¤‡ä¸Šä¼ è·¯å¾„å˜é‡
          YEAR=$(date +'%Y')
          MONTH=$(date +'%m')
          DATE_STR=$(date +'%Y%m%d-%H%M%S')
          
          # é”™è¯¯æ ‡è®°
          HAS_ERROR=false

          echo "Starting multi-database backup process..."
          
          # ä½¿ç”¨ jq è§£æ JSON å¹¶å¾ªç¯å¤„ç†
          # æ³¨æ„ï¼šBase64 ç¼–ç ç”± jq å¤„ç†å¯èƒ½å¤ªå¤æ‚ï¼Œç›´æ¥å¾ªç¯è¯»å–
          # ä¸ºäº†å¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼Œä½¿ç”¨ jq -c è¾“å‡ºæ¯ä¸€è¡Œ compact JSON
          echo "$DB_BACKUP_CONFIG" | jq -c '.[]' | while read -r db_item; do
            # è§£æ name å’Œ url
            DB_NAME=$(echo "$db_item" | jq -r '.name')
            DB_URL=$(echo "$db_item" | jq -r '.url')
            
            if [ -z "$DB_NAME" ] || [ -z "$DB_URL" ]; then
              echo "âš ï¸  Skipping invalid config item: $db_item"
              continue
            fi
            
            FILENAME="${DB_NAME}-backup-${DATE_STR}.sql.gz"
            S3_PATH="s3://${BUCKET_NAME}/${YEAR}/${MONTH}/${FILENAME}"
            
            echo "---------------------------------------------------"
            echo "ğŸ—„ï¸  Processing database: $DB_NAME"
            echo "ğŸ“„ Target filename: $FILENAME"
            
            # ä½¿ç”¨å­ shell æ‰§è¡Œå¤‡é¿å…ç¯å¢ƒå˜é‡æ±¡æŸ“ï¼Œå…è®¸å•ä¸ªå¤±è´¥ç»§ç»­
            (
              set -o pipefail
              # å¤‡ä»½å¹¶å‹ç¼©
              pg_dump "$DB_URL" --format=plain --no-owner --no-acl --clean --if-exists | gzip -9 > "$FILENAME"
              
              # ä¸Šä¼ 
              aws s3 cp "$FILENAME" "$S3_PATH" --endpoint-url "$R2_ENDPOINT"
              
              echo "âœ… Backup success for $DB_NAME"
              
              # æ¸…ç†
              rm -f "$FILENAME"
            ) || {
              echo "âŒ Backup FAILED for $DB_NAME"
              HAS_ERROR=true
            }
            
          done
          
          # å¦‚æœæœ‰ä»»ä½•ä¸€ä¸ªå¤±è´¥ï¼Œæœ€ç»ˆè®©æ­¥éª¤å¤±è´¥ï¼Œä»¥ä¾¿å‘é€é€šçŸ¥ï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
          if [ "$HAS_ERROR" = true ]; then
            echo "---------------------------------------------------"
            echo "âŒ One or more backups failed."
            exit 1
          else
            echo "---------------------------------------------------"
            echo "ğŸ‰ All backups completed successfully."
          fi
